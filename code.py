# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10NQpAfy7E5YI-aCzxK20Xb1UtYe_MxcD
"""

"""# Initializing
Start SparkSession, load data and remove forbidden columns.
Check if the dataframe contains all the expected columns.
"""

from pyspark.sql import SparkSession
import warnings
from pyspark.sql.types import StringType,BooleanType,IntegerType
from pyspark.sql.functions import *


spark = SparkSession \
    .builder \
    .appName("AssignmentApp") \
    .master("local[*]") \
    .getOrCreate()

# Import dataset.csv
filepath = "./dataset.csv"
datasheet = spark.read.csv(filepath, header=True, inferSchema=True)

datasheet.show()

# Check if all of the expected columns exist in the dataframe and remove unexpected columns
expected_columns = ['Year',
                    'Month',
                    'DayofMonth',
                    'DayOfWeek',
                    'DepTime',
                    'CRSDepTime',
                    'ArrTime',
                    'CRSArrTime',
                    'UniqueCarrier',
                    'FlightNum',
                    'TailNum',
                    'ActualElapsedTime',
                    'CRSElapsedTime',
                    'AirTime',
                    'ArrDelay',
                    'DepDelay',
                    'Origin',
                    'Dest',
                    'Distance',
                    'TaxiIn',
                    'TaxiOut',
                    'Cancelled',
                    'CancellationCode',
                    'Diverted',
                    'CarrierDelay',
                    'WeatherDelay',
                    'NASDelay',
                    'SecurityDelay',
                    'LateAircraftDelay']

missing_columns = set(expected_columns) - set(datasheet.columns)
if missing_columns:
    raise ValueError(f"Missing columns in the input data: {', '.join(missing_columns)}")

different_columns = set(datasheet.columns) - set(expected_columns)

if different_columns:
    datasheet = datasheet.select(*expected_columns)
    raise warnings.warn(f"Unexpected columns in the input data: {', '.join(different_columns)}")

# Drop forbidden variables EXCEPT Diverted. It will be used for the data cleaning and removed later on!
df = datasheet.drop("ArrTime", "ActualElapsedTime", "AirTime", "TaxiIn", "CarrierDelay", "WeatherDelay", "NASDelay", "SecurityDelay", "LateAircraftDelay")
df.show()

"""Some of the Null values are encoded as a String "NA". We replace all these values by "None" for all of the columns that are supposed to be integer.

Then, all the columns are converted into the datatype they are supposed to have, raising exceptions if the data provided does not allow this.
"""

# Replace String "NA" by None for all integer columns
integer_columns = set(df.columns) - set(['UniqueCarrier', 'TailNum', 'Origin', 'Dest', 'CancellationCode', 'Cancelled', 'Diverted'])
for col_name in integer_columns:
    df = df.withColumn(col_name, when(col(col_name) == "NA", None).otherwise(col(col_name)))
df.show()

# Convert corresponding columns to integer
for col_name in integer_columns:
    try:
      df = df.withColumn(col_name, df[col_name].cast(IntegerType()))
    except:
        raise ValueError(f"Column '{col_name}' has incorrect data type. Expected: integer, Actual: {df.schema[col_name].dataType}")
df.printSchema()

# Convert corresponding columns to boolean
bool_columns = ['Diverted', 'Cancelled']
for col_name in bool_columns:
    try:
      df = df.withColumn(col_name, df[col_name].cast(BooleanType()))
    except:
        raise ValueError(f"Column '{col_name}' has incorrect data type. Expected: bool, Actual: {df.schema[col_name].dataType}")
df.printSchema()

# Convert corresponding columns to String
string_columns = ['UniqueCarrier', 'TailNum', 'Origin', 'Dest', 'CancellationCode']
for col_name in string_columns:
    try:
      df = df.withColumn(col_name, df[col_name].cast(StringType()))
    except:
        raise ValueError(f"Column '{col_name}' has incorrect data type. Expected: String, Actual: {df.schema[col_name].dataType}")
df.printSchema()

"""Now we have a dataset where all of the values are converted to the correct datatype, making sure that all of the necessary columns exist and that unexpected columns were removed

# Data Preprocessing
- Select useful variables
- Transform variables
- Derive new variables
"""

from pyspark.ml.stat import *
from pyspark.ml.feature import VectorAssembler

import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt

# create df of null value count
null_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])
null_values.show()

# Remove Year, CRSDepTime, CRSArrTime, FlightNum, TailNum, CancellationCode
df2 = df.drop("CRSDepTime", "CRSArrTime", "FlightNum", "TailNum", "CancellationCode")

# Add leading zeros for DepTime
df2 = df2.withColumn("DepTime", lpad(col("DepTime"), 4, "0"))

# Split DepTime to DepHour and DepMinute
df3 = df2.withColumn("DepHour",regexp_replace(col('DepTime'),r'(\d\d$)','')).withColumn("DepMinute", regexp_extract(col('DepTime'), r'(\d\d$)',1))
df3 = df3.drop("DepTime")

# Transform DepHour and DepMinute to integer
df3 = df3.withColumn("DepHour", df3["DepHour"].cast(IntegerType()))
df3 = df3.withColumn("DepMinute", df3["DepMinute"].cast(IntegerType()))
df3.printSchema()

df3.show()

# create df of null value count
null_values = df3.select([count(when(col(c).isNull(), c)).alias(c) for c in df3.columns])
null_values.show()

df3a = df3.where((df3.Cancelled == True) | (df3.Diverted == True))
null_values2 = df3a.select([count(when(col(c).isNull(), c)).alias(c) for c in df3a.columns])
null_values2.show()

print("There are " + str(df3a.count()) + " entries where the flight was cancelled or diverted")

"""We can see, most of the Null values are in columns where the flight was diverted or cancelled.
Also, in our sampled subset at least, all of the rows where the flight was diverted or cancelled have no value for the
Target "ArrDelay"

This means, that all the flights that got cancelled or diverted cannot be predicted by our model,
we do not have any data about the ArrDelay because these flights never arrive on their original destination.
Other potential missing values, in our sample there are unexplained Null values only in Distance and TaxiOut, will be dealt with later on
"""

# Drop cancelled and diverted flights from dataframe and remove "Cancelled" and "Diverted" columns
df4 = df3.where((df3.Cancelled == False) & (df3.Diverted == False))

df4 = df4.drop("Cancelled", "Diverted")

# create df of null value count
null_values3 = df4.select([count(when(col(c).isNull(), c)).alias(c) for c in df4.columns])
null_values3.show()

# Create column lists
integer_columns = ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSElapsedTime', 'ArrDelay', 'DepDelay', 'Distance', 'TaxiOut', 'DepHour', 'DepMinute']
string_columns = ['UniqueCarrier', 'Origin', 'Dest']

# Remove all of the rows where categorical values or ArrDelay is null, these values should not be imputed
df5 = df4.na.drop(subset=['Month', 'DayOfMonth', 'DayOfWeek', 'DepHour', 'DepMinute', 'UniqueCarrier', 'Origin', 'Dest', 'ArrDelay'])

# Show a warning if more than 10% of all rows were removed
total_rows_before = df4.count()
total_rows_after = df5.count()
percentage_removed = (total_rows_before - total_rows_after) / total_rows_before * 100
if percentage_removed > 10:
    warnings.warn(f"Removing more than 10% of rows based on null values in columns 'ArrDelay' and the categorical columns.")

#removed_rows = df4.subtract(df5)
#removed_rows.show()

"""## Exploratory Data Analysis

**ArrDelay Exploratory Analysis**


*   Histogram
*   BoxPlot
*   Outlier Analysis

### Histogram
"""

#Histogram
arr_delay_stats = df5.select("ArrDelay").summary("mean", "50%", "min", "max", "stddev").withColumnRenamed("ArrDelay", "ArrDelay_Stats")
arr_delay_stats.show(truncate=False)

aux1 = df5.withColumn("ArrDelay", col("ArrDelay").cast("int"))

# Extract ArrDelay values for the histogram
arr_delay_values = aux1.select("ArrDelay").rdd.flatMap(lambda x: x).collect()

# Plot the histogram using Matplotlib
plt.hist(arr_delay_values, bins=range(-30, 31, 5), edgecolor='black')
plt.title('ArrDelay Histogram')
plt.xlabel('Arrival Delay')
plt.ylabel('Frequency')
plt.show()

"""For our random sample, the average delay is positive, indicating overall delays in the dataset. The
 median being 0 suggests that a significant portion of flights experience minimal or no delays, while the maximum value of 1625 indicates the presence of extreme delays.

### Boxplot
"""

# Create list of ArrDelay values

arr_delay_list = df5.select("ArrDelay").collect()

# Calculate quartiles and IQR
quantiles = df5.stat.approxQuantile("ArrDelay", [0.25, 0.75], 0.01)
Q1 = quantiles[0]
Q3 = quantiles[1]
IQR = Q3 - Q1

# Define lower and upper bounds to identify outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
# outliers = df5.filter((col("ArrDelay") < lower_bound) | (col("ArrDelay") > upper_bound))
# Define lower and upper bounds for trimming 2.5% from the bottom and top
trim_percentage = 0.025
lower_bound_2p5 = df5.approxQuantile("ArrDelay", [trim_percentage], 0.01)[0]
upper_bound_2p5 = df5.approxQuantile("ArrDelay", [1 - trim_percentage], 0.01)[0]

# Identify values to keep
outlier_treatment = df5.filter((df5['ArrDelay'] >= lower_bound_2p5) & (df5['ArrDelay'] <= upper_bound_2p5))

# Convert the list to a Pandas DataFrame for plotting
arr_delay_pd = pd.DataFrame(arr_delay_list, columns=["ArrDelay"])

# Create a box plot
plt.boxplot(arr_delay_pd["ArrDelay"], vert=False)
plt.title("Box Plot of ArrDelay")
plt.xlabel("ArrDelay")
from pyspark.sql import functions as F
#from pyspark.sql.window import Windowplt.show()

# Calculate mean and standard deviation of ArrDelay
mean_arr_delay = df5.agg({"ArrDelay": "mean"}).collect()[0][0]
stddev_arr_delay = df5.agg({"ArrDelay": "stddev"}).collect()[0][0]

# Calculate z-score for ArrDelay
df_with_zscore = df5.withColumn("z_score", (col("ArrDelay") - mean_arr_delay) / stddev_arr_delay)

df_with_zscore.show()

outliers_threshold = 2.0
outliers_df = df_with_zscore.filter((col("z_score") > outliers_threshold) | (col("z_score") < -outliers_threshold))
df5_ot = df.join(outliers_df, "ArrDelay", "left_anti")

df5_ot.describe("ArrDelay").show()

#Getting or limits to use in the training outliers data
upper_limit = df5_ot.agg({"ArrDelay": "max"}).collect()[0]["max(ArrDelay)"]
lower_limit = df5_ot.agg({"ArrDelay": "min"}).collect()[0]["min(ArrDelay)"]

"""The removal of outliers has had a notable impact on the mean and standard deviation. Before trimming, the mean arrival delay was 0.52 minutes, and after trimming, it reduced to 16 minutes. Similarly, the standard deviation decreased from 30.01 to 12.94, indicating a reduction in the variability of the data.

These changes indicate that outliers were influencing the summary statistics, and their removal has resulted in a shift in the central tendency and dispersion of the "ArrDelay" variable.

Carries Exploratory Analysis


*   Summary
*   Box-plot
*   Skweness and kurtosis
"""

df5_pandas = df5.toPandas()

# Box plot
plt.figure(figsize=(12, 6))
sns.boxplot(x='UniqueCarrier', y='ArrDelay', data=df5_pandas)
plt.title('Box Plot of Arrival Delay by Unique Carrier')
plt.xlabel('Unique Carrier')
plt.ylabel('Arrival Delay')
plt.show()

df5_ot = df5_ot.withColumn("ArrDelay", col("ArrDelay").cast("int"))

# Calculate summary statistics for each carrier
carrier_summary = df5.groupBy("UniqueCarrier").agg(
    mean("ArrDelay").alias("AverageDelay"),
    min("ArrDelay").alias("MinDelay"),
    max("ArrDelay").alias("MaxDelay"),
    stddev("ArrDelay").alias("StdDevDelay")
)

# Display the result
carrier_summary.orderBy(desc("AverageDelay")).show()

#Check for normality before ANOVA
from pyspark.sql.functions import skewness, kurtosis

# Skewness & Kurtosis
skew_kurt = df5.select(
    skewness(df5['ArrDelay']).alias('skewness'),
    kurtosis(df5['ArrDelay']).alias('kurtosis')
)

skew_kurt.show()

"""Variability Among Carriers: There is variability in the average arrival delay among different carriers. Some have positive average delays (indicating they are often late), while others have negative average delays (indicating they often arrive early).

Range of Delays: The minimum and maximum delays show a wide range for all carriers, from -23 to 76 minutes, suggesting that extreme early arrivals and delays can happen with any carrier.

**ANOVA**

Analyzing the variables with an F-Test to determine, if they are useful predictors for ArrDelay.
"""

## Does Carrier have a Statstical Significance on ArrDelay
pandas_df = df5.select('UniqueCarrier', 'ArrDelay').toPandas()

import pandas as pd
from scipy import stats

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('UniqueCarrier')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('Year', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('Year')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('Month', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('Month')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('DayofMonth', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('DayofMonth')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('DayofWeek', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('DayofWeek')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('CRSElapsedTime', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('CRSElapsedTime')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('Distance', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('Distance')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('DepHour', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('DepHour')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('DepMinute', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('DepMinute')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('Dest', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('Dest')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

pandas_df = df5.select('Origin', 'ArrDelay').toPandas()

# Perform ANOVA
anova_results = stats.f_oneway(*[group['ArrDelay'].dropna().values for name, group in pandas_df.groupby('Origin')])

print(f"F-Value: {anova_results.statistic}, P-Value: {anova_results.pvalue}")

"""As we can see, all of our selected variables are relevant according to the Anova F-Test. This means, that we should include all of them in our models.

### Analyze the mean by group
"""

gm = df5_ot.withColumn("ArrDelay", col("ArrDelay").cast("int"))

# Group by DayOfMonth and calculate average delay
daily_delays = gm.groupBy("DayOfMonth").agg(mean("ArrDelay").alias("AverageDelay"))

# Display the result
daily_delays.sort('DayOfMonth').show(31)

"""We can see that the effect of DayOfMonth on ArrDelay is not linear and the Anova test showed that DayOfMonth is significant. Hence, we will try using DayOfMonth as a factor later on.

### Monthly delay
"""

# Group by month and calculate average delay
monthly_delays = gm.groupBy("Month").agg(mean("ArrDelay").alias("AverageDelay"))

# Display the result
monthly_delays.sort('Month').show()

df_selected_years = df5_ot.filter(col("Year").between(2004, 2009))

# Aggregate delays over months and years
monthly_delay = df_selected_years.groupBy("Year", "Month").agg(mean("ArrDelay").alias("AvgArrDelay"))
monthly_delay = monthly_delay.orderBy("Year", "Month")

# Convert the DataFrame to Pandas for local plotting
monthly_delay_pd = monthly_delay.toPandas()
# Plot monthly average delay for each year
plt.figure(figsize=(12, 6))

for year_val in monthly_delay_pd["Year"].unique():
    year_data = monthly_delay_pd[monthly_delay_pd["Year"] == year_val]
    plt.plot(year_data["Month"], year_data["AvgArrDelay"], marker='o', label=str(year_val))

plt.title('Monthly Average Arrival Delay Comparison (2004-2008)')
plt.xlabel('Month')
plt.ylabel('Average Arrival Delay')
plt.xticks(range(1, 13), labels=[f'Month {i}' for i in range(1, 13)])
plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')

plt.show()

"""There is a noticeable seasonal variation in average arrival delays. For example, in multiple years, there is a tendency for higher delays in the summer months (June, July) and towards the end of the year (November, December). This could be influenced by increased travel during holidays and vacation periods.

In our sample, there also seems to be a noticeable difference for each year.

We can conclude that in might be sensible to include Month and Year as a factor for our model, as there does not seem to be a linear relationship between Month and ArrDelay, but still Month has a noticeable impact according to the Anova test.

### Multivariate Analysis
Correlation between independent variables
"""

from pyspark.ml.feature import VectorAssembler

# Calculate correlation matrix without ArrDelay
dep_integer_columns = integer_columns.copy()
dep_integer_columns.remove('ArrDelay')
# Assemble the columns into a single vector column
vector_assembler = VectorAssembler(inputCols=dep_integer_columns, outputCol="features", handleInvalid="skip")
assembled_df = vector_assembler.transform(df5).select("features")

# Compute the correlation matrix
matrix = Correlation.corr(assembled_df, "features").collect()[0][0]
corr_df = pd.DataFrame(matrix.toArray(), columns=dep_integer_columns, index=dep_integer_columns)

sns.set(style="white")  # Set the style of the visualization
plt.figure(figsize=(10, 8))  # Set the size of the plot
mask = np.triu(np.ones_like(corr_df, dtype=bool))

# Create a heatmap with annotations and a color bar
sns.heatmap(corr_df, mask=mask, annot=True, cmap="coolwarm")

plt.title("Correlation Matrix Heatmap")
plt.show()

"""As we can observe, the variables 'CRSElapsedTime' and 'Distance' are highly correlated. In our sample, the column 'Distance' contains some Null values, hence we decide to drop 'Distance' instead of 'CRSElapsedTime'.

The other variables are correlated, but not to a concerning degree and hence are kept in the dataset.
"""

# Drop column Distance
df6 = df5.drop('Distance')  #this is kept to run validations with the full data
integer_columns.remove('Distance')

"""Now, we analyze the correlation between the dependent variables and the predictor variable 'ArrDelay'."""

# Create new correlation matrix including ArrDelay
# Assemble the columns into a single vector column
vector_assembler = VectorAssembler(inputCols=integer_columns, outputCol="features", handleInvalid="skip")
assembled_df = vector_assembler.transform(df5).select("features")

# Compute the correlation matrix
matrix = Correlation.corr(assembled_df, "features").collect()[0][0]
corr_df = pd.DataFrame(matrix.toArray(), columns=integer_columns, index=integer_columns)

sns.set(style="white")  # Set the style of the visualization
plt.figure(figsize=(10, 8))  # Set the size of the plot
mask = np.triu(np.ones_like(corr_df, dtype=bool))

# Create a heatmap with annotations and a color bar
sns.heatmap(corr_df, mask=mask, annot=True, cmap="coolwarm")

plt.title("Correlation Matrix Heatmap")
plt.show()

"""As we can see, the variables 'DepDelay', 'TaxiOut' and 'DepHour' have the highest correlation with the target 'ArrDelay'. This means that these variables have a higher influence on the target variable than the others and should be kept in the model.

However, in our sampled subset, 'TaxiOut' has a lot of missing values - almost a third of all values are missing because this variable was not measured in the earlier years. We can deal with this issue by either imputing the values, dropping the rows or dropping the columns.

Dropping the rows is not an option because there are too many missing values in this column, our dataset might be heavily biased afterwards. Dropping the column is also not an option because, as we just discovered, the variable is important to predict the target variable. Hence, we decide to **impute** the values.
"""

# Set final dataframe
fdf = df6
fdf.describe('ArrDelay').show()

"""## Transformation"""

from pyspark.ml.feature import Imputer
from pyspark.ml.feature import StringIndexer, OneHotEncoder
from pyspark.ml.feature import Normalizer, VectorAssembler
from pyspark.ml import Pipeline

"""### Encode categorical variables

Two different pipelines are created. The first, where the categorical variables that were found to be non-linear towards ArrDelay, namely "Month", "DayofMonth", "DayOfWeek", as well as the string variables, "UniqueCarrier", "Origin" and "Dest", are one-hot encoded.

The second one, where only the string variables are converted by an Indexer.
"""

# Categorical encoder
# StringIndexer for converting strings to indices
indexer_unique_carrier = StringIndexer(inputCol="UniqueCarrier", outputCol="UniqueCarrier_Index")
indexer_origin = StringIndexer(inputCol="Origin", outputCol="Origin_Index")
indexer_dest = StringIndexer(inputCol="Dest", outputCol="Dest_Index")

# OneHotEncoder for converting indices to binary vectors
encoder_unique_carrier = OneHotEncoder(inputCols=["UniqueCarrier_Index"], outputCols=["UniqueCarrier_Vec"])
encoder_origin = OneHotEncoder(inputCols=["Origin_Index"], outputCols=["Origin_Vec"])
encoder_dest = OneHotEncoder(inputCols=["Dest_Index"], outputCols=["Dest_Vec"])
encoder_month = OneHotEncoder(inputCols=["Month"], outputCols=["Month_Vec"])
encoder_DayofMonth = OneHotEncoder(inputCols=["DayofMonth"], outputCols=["DayofMonth_Vec"])
encoder_DayofWeek = OneHotEncoder(inputCols=["DayOfWeek"], outputCols=["DayofWeek_Vec"])

# Pipeline
encoder_pipeline_ohe = Pipeline(stages=[
    indexer_unique_carrier,
    indexer_origin,
    indexer_dest,
    encoder_unique_carrier,
    encoder_origin,
    encoder_dest,
    encoder_month,
    encoder_DayofMonth,
    encoder_DayofWeek
])

# Fit and Transform
encoder_fdf_ohe = encoder_pipeline_ohe.fit(fdf)
transformed_df_ohe = encoder_fdf_ohe.transform(fdf)
transformed_df_ohe.show()

# Pipeline without the variables without Month, DayofMonth, DayOfWeek as one-hot-encoded variables

# Pipeline
encoder_pipeline = Pipeline(stages=[
    indexer_unique_carrier,
    indexer_origin,
    indexer_dest
])

# Fit and Transform
encoder_fdf = encoder_pipeline.fit(fdf)
transformed_df = encoder_fdf.transform(fdf)
transformed_df.show()

# Delete old version variables
df_numerical_ohe = transformed_df_ohe.drop("UniqueCarrier","Dest", "Origin","UniqueCarrier_Index","Origin_Index","Dest_Index", "Month", "DayofMonth",
                                   "DayOfWeek")
df_numerical_ohe.show()

df_numerical_ohe = df_numerical_ohe.withColumnsRenamed({"UniqueCarrier_Vec":"UniqueCarrier",
                                                        "Origin_Vec":"Origin",
                                                        "Dest_Vec":"Dest",
                                                        "Month_Vec":"Month",
                                                        "DayofMonth_Vec":"DayofMonth",
                                                        "DayofWeek_Vec":"DayOfWeek"})
df_numerical_ohe.show()

df_numerical = transformed_df.drop("UniqueCarrier","Dest", "Origin")
df_numerical.show()

df_numerical = df_numerical.withColumnsRenamed({"UniqueCarrier_Index":"UniqueCarrier",
                                                        "Origin_Index":"Origin",
                                                        "Dest_Index":"Dest"})
df_numerical.show()

"""Potential missing categorical variables have already been removed. Now, we define the numerical columns that are eligible to be imputed."""

# Define imputable columns
imputable_columns = ['CRSElapsedTime', 'DepDelay', 'TaxiOut']

one_hot_encoded_df = df_numerical_ohe.drop('CRSElapsedTime', 'ArrDelay', 'DepDelay', 'TaxiOut', 'Year', 'DepMinute', 'DepHour')
one_hot_encoded_df.show()

# Define numerical Imputer
# In case not all the null values in the numerical columns are removed, impute by mean
numerical_imputer = Imputer()
numerical_imputer.setInputCols(imputable_columns)
numerical_imputer.setOutputCols(imputable_columns)
numerical_imputer.setStrategy("mean")
numerical_imputer.setMissingValue(None)

# Apply Imputer on OHE data
imputer = numerical_imputer.fit(df_numerical_ohe)
df_numerical_ohe = imputer.transform(df_numerical_ohe)
df_numerical_ohe.show()

# Apply Imputer on regular data
imputer2 = numerical_imputer.fit(df_numerical)
df_numerical = imputer2.transform(df_numerical)
df_numerical.show()

# Define Feature columns
feature_columns = df_numerical.drop("ArrDelay").columns

# Other transformations
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")

normalizer = Normalizer(inputCol="features", outputCol="normFeatures", p=1.0)

"""# Machine Learning Model"""

from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor
from pyspark.ml.regression import LinearRegression
from pyspark.ml import Pipeline

"""## Linear Regression

"""

lr = LinearRegression(labelCol="ArrDelay", featuresCol="normFeatures")

# creatio of a pipeline for the linear regression
pipeline_lr = Pipeline(stages=[assembler, normalizer, lr])

"""## Decision Tree"""

dt = DecisionTreeRegressor(featuresCol="normFeatures", labelCol="ArrDelay")

# creation of a pipeline for the decision tree
pipeline_df = Pipeline(stages=[assembler, normalizer, dt])

"""# Validation"""

# evaluation with rmse
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# Split the data into training and testing sets
split = df_numerical.randomSplit([0.8, 0.2], seed=42)
train_data = split[0]
test_data = split[1]
train_data.show()

# Split the data with Month, DayOfWeek, DayofMonth as one-hot-encoded variables
split_ohe = df_numerical_ohe.randomSplit([0.8, 0.2], seed=42)
train_data_ohe = split_ohe[0]
test_data_ohe = split_ohe[1]
train_data_ohe.show()

# Split the data (with outliers limit) into training and testing sets
df_numerical_ot = df_numerical.filter((col("ArrDelay") >= lower_limit) & (col("ArrDelay") <= upper_limit)) #Limits previously established for Outliers
split_ot = df_numerical_ot.randomSplit([0.8, 0.2], seed=42)
train_data_ot = split_ot[0]
train_data_ot.describe('ArrDelay').show()

# Set up the parameter grids
paramGrid_lr = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \
    .build()

# Set up the parameter grids
paramGrid_dt = ParamGridBuilder() \
    .addGrid(dt.maxDepth, [5, 10, 15]) \
    .addGrid(dt.maxBins, [10, 20, 30]) \
    .build()

# Set up the evaluator
evaluator = RegressionEvaluator(labelCol="ArrDelay", predictionCol="prediction", metricName="rmse")

# Set up cross-validation
cv_lr = CrossValidator(estimator=pipeline_lr, estimatorParamMaps=paramGrid_lr, evaluator=evaluator, numFolds=5)
cv_dt = CrossValidator(estimator=pipeline_df, estimatorParamMaps=paramGrid_dt, evaluator=evaluator, numFolds=5)

"""## Decision Tree Evaluation"""

# Fit the model
model_dt = cv_dt.fit(train_data)

# Evaluate the model
predictions_dt = model_dt.transform(test_data)

rmse_dt = evaluator.evaluate(predictions_dt)

print(f"The RMSE of the Decision Tree Model is {rmse_dt}")

"""## Linear Regression Evaluation

Let's fit the linear regression model on the normal dataset
"""

# Fit the model
model_lr = cv_lr.fit(train_data)

# Evaluate the Linear Regression model
predictions_lr = model_lr.transform(test_data)

rmse_lr = evaluator.evaluate(predictions_lr)

print(f"The RMSE of the Linear Regression Model is {rmse_lr}")

"""Fitting of the model with the variables UniqueCarrier, Dest, Origin, Month, DayOfWeek, DayofMonth one-hot-encoded to take their non-linearity into consideration."""

# Fit the model
model_lr_ohe = cv_lr.fit(train_data_ohe)

# Evaluate the Linear Regression model
predictions_lr_ohe = model_lr_ohe.transform(test_data_ohe)

rmse_lr_ohe = evaluator.evaluate(predictions_lr_ohe)

print(f"The RMSE of the Linear Regression Model is {rmse_lr_ohe}")

"""### Residual Analysis
We perform a residual analysis to check, if there is a pattern in the residuals. This is only sensible for the Linear Regression model.

We check the residuals for the parameters that we expect to have the highest impact on ArrDelay: With a correlation of at least 0.01
"""

# Generate residuals for Linear Regression
residuals_lr = predictions_lr.select("DepDelay", "TaxiOut", "DepHour", "ArrDelay", "prediction") \
    .withColumn("residuals", col("ArrDelay") - col("prediction"))

# Convert PySpark DataFrame to Pandas DataFrame for visualization
lr_residuals_pd = residuals_lr.select("residuals", "DepDelay", "TaxiOut", "DepHour").toPandas()

# Create subplots
fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 8))

# Scatterplot 1
axes[0].scatter(lr_residuals_pd["DepDelay"], lr_residuals_pd["residuals"])
axes[0].set_title("Scatter Plot of Residuals against DepDelay")

# Scatterplot 2
axes[1].scatter(lr_residuals_pd["TaxiOut"], lr_residuals_pd["residuals"])
axes[1].set_title("Scatter Plot of Residuals against TaxiOut")

# Scatterplot 3
axes[2].scatter(lr_residuals_pd["DepHour"], lr_residuals_pd["residuals"])
axes[2].set_title("Scatter Plot of Residuals against DepHour")

# Adjust layout
plt.tight_layout()

# Show the plots
plt.show()

"""We can observe that the residuals still contain some pattern, which we should try to address as this means that some of the pattern in the data is not captured by the linear model. Also, we can see that there are influential outliers.

Decision Tree models are usually not sensitive to outliers and do not require linearity of data.

Hence, we try to reduce the impact of outliers for the Linear Regressor and see, if we can improve our model this way.

### Outlier robust Linear Regression Model
Our previous results indicated an issue with outliers in our model. Hence, we retrain the model with a subset where outliers have been removed to see, if this increases the performance of our LR model.
"""

# Fit the model
model_lr_ot = cv_lr.fit(train_data_ot)

# Evaluate the Linear Regression model
predictions_lr_ot = model_lr_ot.transform(test_data)

rmse_lr_ot = evaluator.evaluate(predictions_lr_ot)

print(f"The RMSE of the Linear Regression Model is {rmse_lr_ot}")

"""#### Residual Analysis of the outlier-sensitive model"""

# Generate residuals for Linear Regression
residuals_lr_ot = predictions_lr_ot.select("DepDelay", "TaxiOut", "DepHour", "ArrDelay", "prediction") \
    .withColumn("residuals", col("ArrDelay") - col("prediction"))

# Convert PySpark DataFrame to Pandas DataFrame for visualization
lr_ot_residuals_pd = residuals_lr_ot.select("residuals", "DepDelay", "TaxiOut", "DepHour").toPandas()

# Create subplots
fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 8))

# Scatterplot 1
axes[0].scatter(lr_ot_residuals_pd["DepDelay"], lr_ot_residuals_pd["residuals"])
axes[0].set_title("Scatter Plot of Residuals against DepDelay")

# Scatterplot 2
axes[1].scatter(lr_ot_residuals_pd["TaxiOut"], lr_ot_residuals_pd["residuals"])
axes[1].set_title("Scatter Plot of Residuals against TaxiOut")

# Scatterplot 3
axes[2].scatter(lr_ot_residuals_pd["DepHour"], lr_ot_residuals_pd["residuals"])
axes[2].set_title("Scatter Plot of Residuals against DepHour")

# Adjust layout
plt.tight_layout()

# Show the plots
plt.show()

"""We can observe that trimming the outliers, at least in the way how it was performed here, did not 
improve our model - in the contrary. 

This means that our Linear Regression model with One-Hot Encoding was the best model.
"""

spark.stop()